{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import cve_diseases\n",
    "from nets import inception_resnet_v2\n",
    "from nets import alexnet\n",
    "from nets import nets_factory\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = \"inception_resnet_v2\"\n",
    "model2 = \"inception_v3\"\n",
    "\n",
    "dataset_name = \"cve_diseases\"\n",
    "dataset_split_name = \"train\"\n",
    "dataset_dir = \"tmp/\"\n",
    "batch_size = 16\n",
    "max_number_of_steps = 50\n",
    "train_dir = \"./tmp/cve_diseases-models/test-model6/\"\n",
    "\n",
    "cnn1 = nets_factory.get_network_fn(\n",
    "    model1,\n",
    "    num_classes=None,\n",
    "    weight_decay=0.00004,\n",
    "    is_training=True)\n",
    "cnn2 = nets_factory.get_network_fn(\n",
    "    model2,\n",
    "    num_classes=None,\n",
    "    weight_decay=0.00004,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jonet(images):\n",
    "\n",
    "    net1, end_points1 = cnn1(images)\n",
    "    #net2, end_points2 = cnn2(images)\n",
    "#    for endpoint in end_points1: print(\"Endpoint: %s - Shape: %s\" % (endpoint, end_points1[endpoint].get_shape())) \n",
    "#    print(\"###############################\") \n",
    "#    for endpoint in end_points2: print(\"Endpoint: %s - Shape: %s\" % (endpoint , end_points2[endpoint].get_shape()))\n",
    "#    if 'Conv2d_7b_1x1' in end_points1:\n",
    "#        if 'Mixed_7c' in end_points2:\n",
    "            \n",
    " #           net1 = end_points1['Conv2d_7b_1x1']\n",
    "           # net1 = slim.conv2d(net1, 256, 1)\n",
    "           # net2 = end_points2['alexnet_v2/conv5']\n",
    "  #          net2 = end_points2['Mixed_7c']\n",
    "  #  net = tf.concat((net1, net2), 3)\n",
    "    print(\"Net1 shape: %s\" % net1.get_shape())\n",
    "    print(\"Endpoint Conv2d_7b_1x1 shape: %s\" % end_points1['Conv2d_7b_1x1'])\n",
    "    net = slim.flatten(net1)\n",
    "    net = slim.dropout(net, 0.8, is_training=True)\n",
    "    net = slim.fully_connected(net, 11, activation_fn=None)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size=8, height=299, width=299, is_training=False):\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset)\n",
    "\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    image = inception_preprocessing.preprocess_image(\n",
    "        image,\n",
    "        height,\n",
    "        width,\n",
    "        is_training)\n",
    "\n",
    "    images, labels = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        allow_smaller_final_batch=True)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1 shape: (?, 1, 1, 1536)\n",
      "Endpoint Conv2d_7b_1x1 shape: Tensor(\"InceptionResnetV2/InceptionResnetV2/Conv2d_7b_1x1/Relu:0\", shape=(?, 8, 8, 1536), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path ./tmp/cve_diseases-models/test-model6/model.ckpt\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1780226"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Select dataset\n",
    "dataset = cve_diseases.get_split('train', dataset_dir)\n",
    "\n",
    "#Load batch\n",
    "images, labels = load_batch(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    is_training=True)\n",
    "\n",
    "# run the image through the model\n",
    "\n",
    "logits = jonet(images)\n",
    "\n",
    "\n",
    "# get the cross-entropy loss\n",
    "one_hot_labels = slim.one_hot_encoding(\n",
    "    labels,\n",
    "    dataset.num_classes)\n",
    "\n",
    "tf.losses.softmax_cross_entropy(\n",
    "    one_hot_labels,\n",
    "    logits)\n",
    "\n",
    "#tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "#    _sentinel=None,\n",
    "#    labels=labels,\n",
    "#    logits=logits,\n",
    "#    dim=-1,\n",
    "#    name=None\n",
    "#)\n",
    "\n",
    "\n",
    "total_loss = tf.losses.get_total_loss()\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "# use RMSProp to optimize\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "# create train op\n",
    "train_op = slim.learning.create_train_op(\n",
    "    total_loss,\n",
    "    optimizer,\n",
    "    summarize_gradients=True)\n",
    "\n",
    "# run training\n",
    "slim.learning.train(\n",
    "    train_op,\n",
    "    logdir=train_dir,\n",
    "    number_of_steps=max_number_of_steps,\n",
    "    log_every_n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper funcs\n",
    "for endpoint in end_points1:\n",
    "    print(\"Endpoint: %s - Shape: %s\" % (endpoint, end_points1[endpoint].get_shape()))\n",
    "print(\"###############################\")\n",
    "for endpoint in end_points2:\n",
    "    print(\"Endpoint: %s - Shape: %s\" % (endpoint , end_points2[endpoint].get_shape()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
