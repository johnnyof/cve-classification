{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import cve_diseases\n",
    "from nets import inception_resnet_v2\n",
    "from nets import alexnet\n",
    "from nets import nets_factory\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = \"inception_resnet_v2\"\n",
    "model2 = \"inception_v3\"\n",
    "\n",
    "dataset_name = \"cve_diseases\"\n",
    "dataset_split_name = \"train\"\n",
    "dataset_dir = \"tmp/\"\n",
    "batch_size = 16\n",
    "max_number_of_steps = 100000\n",
    "train_dir = \"./tmp/cve_diseases-models/model2-ds2/\"\n",
    "\n",
    "cnn1 = nets_factory.get_network_fn(\n",
    "    model1,\n",
    "    num_classes=None,\n",
    "    weight_decay=0.00004,\n",
    "    is_training=True)\n",
    "cnn2 = nets_factory.get_network_fn(\n",
    "    model2,\n",
    "    num_classes=None,\n",
    "    weight_decay=0.00004,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jonet(images):\n",
    "\n",
    "    net1, end_points1 = cnn1(images)\n",
    "    net1 = end_points1['Mixed_7a']\n",
    "    \n",
    "    net2, end_points2 = cnn2(images)\n",
    "    net2 = end_points2['Mixed_7c']\n",
    "    net = tf.concat((net1, net2), 3)\n",
    "    net = slim.flatten(net)\n",
    "    net = slim.dropout(net, 0.8, is_training=True)\n",
    "    net = slim.fully_connected(net, 11, activation_fn=None)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size=8, height=299, width=299, is_training=False):\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset)\n",
    "\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    image = inception_preprocessing.preprocess_image(\n",
    "        image,\n",
    "        height,\n",
    "        width,\n",
    "        is_training)\n",
    "    one_hot_labels = slim.one_hot_encoding(label, dataset.num_classes)\n",
    "\n",
    "\n",
    "    images, labels = tf.train.batch(\n",
    "        [image, one_hot_labels],\n",
    "        batch_size=batch_size,\n",
    "        allow_smaller_final_batch=True)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Select dataset\n",
    "dataset = cve_diseases.get_split('train', dataset_dir)\n",
    "\n",
    "#Load batch\n",
    "images, labels = load_batch(\n",
    "    dataset,\n",
    "    batch_size,\n",
    "    is_training=True)\n",
    "\n",
    "# run the image through the model\n",
    "\n",
    "logits = jonet(images)\n",
    "\n",
    "\n",
    "# get the cross-entropy loss\n",
    "\n",
    "loss = slim.losses.softmax_cross_entropy(logits, labels)\n",
    "total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "#Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(0.00001)\n",
    "\n",
    "\n",
    "\n",
    "predictions = tf.argmax(logits, 1)\n",
    "targets = tf.argmax(labels, 1)\n",
    "\n",
    "correct_prediction = tf.equal(predictions, targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "#Write summary\n",
    "tf.summary.scalar('losses/total_loss', total_loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# create train op\n",
    "train_op = slim.learning.create_train_op(\n",
    "    total_loss,\n",
    "    optimizer,\n",
    "    summarize_gradients=True)\n",
    "\n",
    "# run training\n",
    "slim.learning.train(\n",
    "    train_op,\n",
    "    logdir=train_dir,\n",
    "    number_of_steps=max_number_of_steps,\n",
    "    summary_op=summary_op,\n",
    "    save_summaries_secs=30,\n",
    "    save_interval_secs=30,\n",
    "    log_every_n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper funcs\n",
    "for endpoint in end_points1:\n",
    "    print(\"Endpoint: %s - Shape: %s\" % (endpoint, end_points1[endpoint].get_shape()))\n",
    "print(\"###############################\")\n",
    "for endpoint in end_points2:\n",
    "    print(\"Endpoint: %s - Shape: %s\" % (endpoint , end_points2[endpoint].get_shape()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
