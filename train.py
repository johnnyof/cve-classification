
# coding: utf-8

# IMPORTS

# In[1]:



from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from datasets import cve_diseases
from nets import inception_resnet_v2
from nets import alexnet
from nets import nets_factory
from preprocessing import inception_preprocessing

slim = tf.contrib.slim


# CONFIGURE VARIABLES

# In[2]:


model1 = "inception_resnet_v2"
model2 = "alexnet_v2"

dataset_name = "cve_diseases"
dataset_split_name = "train"
dataset_dir = "tmp/"
batch_size = 32
max_number_of_steps = 60000
train_dir = "./tmp/cve_diseases-models/test-model/"

cnn1 = nets_factory.get_network_fn(
    model1,
    num_classes=None,
    weight_decay=0.00004,
    is_training=True)
cnn2 = nets_factory.get_network_fn(
    model2,
    num_classes=None,
    weight_decay=0.00004,
    is_training=True)


# DEFINE FUNCTIONS

# In[3]:


def jonet(images):

    net1, end_points1 = cnn1(images)
    net2, end_points2 = cnn2(images)
    if 'PreAuxLogits' in end_points1:
        if 'alexnet_v2/conv4' in end_points2:
            
            net1 = end_points1['PreAuxLogits']
           # net1 = slim.conv2d(net1, 256, 1)
            net2 = end_points2['alexnet_v2/conv5']
            net = tf.concat((net1, net2), 3)
            net = slim.flatten(net)
            net = slim.dropout(net, 0.8, is_training=True)
            net = slim.fully_connected(net, 11, activation_fn=None)
            return net


# In[4]:


def load_batch(dataset, batch_size=8, height=299, width=299, is_training=False):
    data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset)

    image, label = data_provider.get(['image', 'label'])

    image = inception_preprocessing.preprocess_image(
        image,
        height,
        width,
        is_training)

    images, labels = tf.train.batch(
        [image, label],
        batch_size=batch_size,
        allow_smaller_final_batch=True)

    return images, labels


# MAIN

# In[5]:



#Select dataset
dataset = cve_diseases.get_split('train', dataset_dir)

#Load batch
images, labels = load_batch(
    dataset,
    batch_size,
    is_training=True)

# run the image through the model

logits = jonet(images)


# get the cross-entropy loss
one_hot_labels = slim.one_hot_encoding(
    labels,
    dataset.num_classes)

slim.losses.softmax_cross_entropy(
    logits,
    one_hot_labels)

#tf.nn.softmax_cross_entropy_with_logits_v2(
#    _sentinel=None,
#    labels=labels,
#    logits=logits,
#    dim=-1,
#    name=None
#)


total_loss = tf.losses.get_total_loss()
tf.summary.scalar('loss', total_loss)

# use RMSProp to optimize
optimizer = tf.train.RMSPropOptimizer(0.001, 0.9)

# create train op
train_op = slim.learning.create_train_op(
    total_loss,
    optimizer,
    summarize_gradients=True)

# run training
slim.learning.train(
    train_op,
    logdir=train_dir,
    number_of_steps=max_number_of_steps,
    log_every_n_steps=100)


# Helper funcs
# for endpoint in end_points1:
#     print("Endpoint: %s - Shape: %s" % (endpoint, end_points1[endpoint].get_shape()))
# print("###############################")
# for endpoint in end_points2:
#     print("Endpoint: %s - Shape: %s" % (endpoint , end_points2[endpoint].get_shape()))
# 
